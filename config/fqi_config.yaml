# @package _global_
# This is a collection of parameters used in FQI and RFQI algorithm. We intergrate them together.
# Default configuration for FQI algorithm
defaults:
  - _self_
  - env/fqi: pendulum
  - env_mods  # New include for environment modifications
  - override hydra/job_logging: default
  - override hydra/hydra_logging: default

# ------------  General settings  ------------
# Device used
device: cuda
# Whether writing summary on Tensorboard
write: false
# Whether priting network loss during training
debug_print: false
# Whether using render mode in gymnasium
render: false
# Whether load model, typically used in evaluation
load_model: false
# Model loading path
load_path: None
# Whether evaluate policy
eval_model: false
# Whether save model during training
save_model: true
# Data path in offline mode
data_path: None

# ------------  Training settings  ------------
# Random seed
seed: 42
# How often (time steps) we evaluate
eval_freq: 1e4
save_interval: 2e3
# max time steps to train FQI
max_trn_steps: 5e5
max_vae_trn_step: 2e5
data_size: 1000000
# use d4rl dataset
d4rl: False
d4rl_v2: False
d4rl_expert: False
# use mixed dataset
mixed: False
# extra comment
comment: ''
# save video
video: False

# mini batch size for networks
batch_size: 1000 
# discount factor
gamma: 0.99 
# target network update rate
tau: 0.005
# weighting for clipped double Q-learning in BCQ
lmbda: 0.75
# max perturbation hyper-parameter for BCQ
phi: 0.1
# epsilon in Adam*
adam_eps: 1e-6
# Adam stepsize*
adam_lr: 1e-3
# learning rate of actor
actor_lr: 1e-3
critic_lr: 1e-3
# number of sampling action for policy (in backup)
n_action: 100
# number of sampling action for policy (in execution)
n_action_execute: 100
# number of training iterations per step
max_iter: 1000

# BCQ-PQL parameter
# "QL": q learning (Q-max) back up, "AC": actor-critic backup
backup: 'QL'
# noise of next action in QL
ql_noise: 0.15
# if true, use percentile for b (beta is the b in paper)
automatic_beta: False
# use x-Percentile as the value of b
beta_percentile: 2.0 
# hardcoded b, only effective when automatic_beta = False
beta: -0.4
# min value of the environment
# empirically set it to be the min of 1000 random rollout.
vmin: 0

# ------------  Environment configurations  ------------
# Whether using reward engineering
reward_adapt: false # only for pendulum and lunarlander
# Whether normalizing reward 
reward_normalize: false
# Whether using robust policy
robust: false
# Radius of TV distance ball
rho: 0.5
# Probability of taking random actions, used in some test cases
random_action_prob: 0.0

# Hydra output directory
hydra:
  run:
    dir: ./outputs/FQI/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ./multirun/FQI/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  job:
    name: ${env_name}_training
    chdir: true  # Change to the output directory




